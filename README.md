# ğŸš‘ Healthcare Data Analytics Pipeline

## ğŸ“Œ Purpose

This project builds a **scalable Big Data pipeline** to process and analyze clinical data from the **MIMIC-III demo dataset**. The pipeline supports both **batch analytics** using **MapReduce** and **Apache Hive**, and aims to extract vital healthcare insights including:

- Average patient age.
- Gender distribution.
- ICU readmission trends.
- Length of stay.
- Mortality rates.
- Top prescriptions and procedures.
- Changes in vital signs during ICU stays.

## ğŸ¯ Objectives

- âœ… Extract raw healthcare data from MIMIC-III CSV files.
- âœ… Clean and preprocess the data using **Python & Pandas**.
- âœ… Convert cleaned CSV files to **Parquet** format using **PySpark**.
- âœ… Store data in **HDFS** inside a Dockerized Hadoop environment.
- âœ… Create **Hive external tables** on top of Parquet files.
- âœ… Perform analytics using **Hive SQL** and **Java-based MapReduce**.

---

## ğŸ” Pipeline Overview
CSV (raw)
â†“
Cleaning via Python (Pandas)
â†“
Parquet Conversion (PySpark)
â†“
Storage in HDFS
â†“
Hive External Tables
â†“
Analytics via Hive SQL + Java MapReduce
---

## ğŸ“‚ Datasets Used (from MIMIC-III Demo)

- `PATIENTS.csv`
- `ADMISSIONS.csv` 
- `DIAGNOSES_ICD.csv` 
- `PROCEDURES_ICD.csv` 
- `PRESCRIPTIONS.csv` 
- `LABEVENTS.csv` 
- `CHARTEVENTS.csv`
- `ICUSTAYS.csv`

---

## ğŸ§¹ Data Cleaning & Transformation

**Scripts used:**

- `clean_patients.py`.
- `convert_csv_to_parquet.py`.
- `convert_parquet_to_csv.py`.

**Cleaning tasks:**

- Remove rows with all null values.
- Normalize date formats and timestamps. 
- Encode categorical fields like gender.
- Fill or flag missing values appropriately.
- Convert cleaned data to efficient **Parquet** format.

---

## ğŸ“¦ Data Storage

All data is stored in **HDFS**, organized as:

- `/user/mimic/input_clean/` â†’ Cleaned CSVs. 
- `/user/mimic/parquet/` â†’ Parquet files.

HDFS runs within a **Dockerized Hadoop** environment for easy deployment and testing.

---

## ğŸ Hive Integration

External **Hive tables** were created over the Parquet files using the `create_hive_tables.sql` script.

ğŸ“„ Location: `sql/create_hive_tables.sql`

---

## ğŸ“Š Analytics

### ğŸ” Hive SQL Queries (in `queries.sql`)

1. Average length of stay per diagnosis. 
2. ICU readmission distribution.
3. Mortality rates by age and gender (filtered to age â‰¤ 85). 
4. ICU length of stay per diagnosis.
5. Relationship between age, length of stay, and mortality. 
6. Top prescribed drugs per diagnosis. 
7. Vital signs trend over days of ICU stay. 

ğŸ“ Results saved to: `results/output_host/`

---

### â˜• Java MapReduce Jobs

**Location:** `mapreduce/patient_stats/PatientStats.java` 
**Output file:** `mapreduce/result/part-r-00000`

Calculated:

- **Average patient age** 
- **Gender distribution** (number of male and female patients)

---

## âš™ï¸ Technologies Used

- **Python** (Pandas, PyArrow) 
- **PySpark** 
- **Java** (MapReduce) 
- **Apache Hadoop** (HDFS + YARN) 
- **Apache Hive** 
- **Docker** 
- **Ubuntu 22.04**

---

## ğŸ“ Project Structure
healthcare-project/
â”œâ”€â”€ database/
â”‚ â”œâ”€â”€ parquet/
â”‚ â”œâ”€â”€ *.csv
â”œâ”€â”€ mapreduce/
â”‚ â”œâ”€â”€ patient_stats/
â”‚ â””â”€â”€ result/
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ create_hive_tables.sql
â”‚ â””â”€â”€ queries.sql
â”œâ”€â”€ results/
â”‚ â””â”€â”€ output_host/
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ clean_patients.py
â”‚ â”œâ”€â”€ convert_csv_to_parquet.py
â”‚ â””â”€â”€ convert_parquet_to_csv.py
â”œâ”€â”€ Docs/
â”‚ â””â”€â”€ USER_MANUAL.md
â”œâ”€â”€ README.md

---

## ğŸ“ Contact

Name: El-Sayed Ehab 
Email: elsayed.ehab0107@gmail.com

---
